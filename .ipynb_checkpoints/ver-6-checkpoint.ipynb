{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "kc4-ppijiTTj"
   },
   "source": [
    "# Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:35:25.620715Z",
     "iopub.status.busy": "2021-07-06T13:35:25.620352Z",
     "iopub.status.idle": "2021-07-06T13:35:25.632416Z",
     "shell.execute_reply": "2021-07-06T13:35:25.631432Z",
     "shell.execute_reply.started": "2021-07-06T13:35:25.620684Z"
    },
    "id": "XrvAu2_TfHkI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:35:30.153421Z",
     "iopub.status.busy": "2021-07-06T13:35:30.153077Z",
     "iopub.status.idle": "2021-07-06T13:35:30.159266Z",
     "shell.execute_reply": "2021-07-06T13:35:30.158241Z",
     "shell.execute_reply.started": "2021-07-06T13:35:30.153387Z"
    },
    "id": "3eeQBY5oiXGB"
   },
   "outputs": [],
   "source": [
    "tokenizer_W = WordPunctTokenizer()\n",
    "\n",
    "# tokenizer is the same in both cases. This is made just for convenience\n",
    "def tokenize_ru(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())\n",
    "\n",
    "def tokenize_en(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "ofb05NHwkKfc"
   },
   "source": [
    "# Word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:35:32.620144Z",
     "iopub.status.busy": "2021-07-06T13:35:32.619821Z",
     "iopub.status.idle": "2021-07-06T13:35:33.020649Z",
     "shell.execute_reply": "2021-07-06T13:35:33.019812Z",
     "shell.execute_reply.started": "2021-07-06T13:35:32.620113Z"
    },
    "id": "nX8SDS88kYai"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:35:34.83591Z",
     "iopub.status.busy": "2021-07-06T13:35:34.835591Z",
     "iopub.status.idle": "2021-07-06T13:35:34.841944Z",
     "shell.execute_reply": "2021-07-06T13:35:34.840872Z",
     "shell.execute_reply.started": "2021-07-06T13:35:34.835883Z"
    },
    "id": "woPSfkcgsLod"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "      self.file_path = file_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = self.file_path\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield tokenize_en(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-03T21:17:16.781536Z",
     "iopub.status.busy": "2021-07-03T21:17:16.781275Z",
     "iopub.status.idle": "2021-07-03T21:19:26.09114Z",
     "shell.execute_reply": "2021-07-03T21:19:26.090274Z",
     "shell.execute_reply.started": "2021-07-03T21:17:16.781511Z"
    },
    "id": "Ri-yro3MmqaT"
   },
   "outputs": [],
   "source": [
    "sentences_en = MyCorpus('../input/400k-sentence/data_ya_hw_3_modif_sentences_en.txt')\n",
    "\n",
    "# default Word2Vec model - CBOW\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py\n",
    "model_en = gensim.models.Word2Vec(sentences=sentences_en, vector_size=256, window=5, min_count=5, workers=4)\n",
    "\n",
    "model_en.wv.save_word2vec_format(\"word2vec_en.wordvectors\")\n",
    "del model_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-03T21:19:34.138486Z",
     "iopub.status.busy": "2021-07-03T21:19:34.138096Z",
     "iopub.status.idle": "2021-07-03T21:21:58.619204Z",
     "shell.execute_reply": "2021-07-03T21:21:58.618257Z",
     "shell.execute_reply.started": "2021-07-03T21:19:34.138445Z"
    },
    "id": "cUeAm2rbwYYk"
   },
   "outputs": [],
   "source": [
    "sentences_ru = MyCorpus('../input/400k-sentence/data_ya_hw_3_modif_sentences_ru.txt')\n",
    "model_ru = gensim.models.Word2Vec(sentences=sentences_ru, vector_size=256, window=5, min_count=5, workers=4)\n",
    "\n",
    "model_ru.wv.save_word2vec_format(\"word2vec_ru.wordvectors\")\n",
    "del model_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "xAb2NO0skTIs"
   },
   "source": [
    "# Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:35:53.990839Z",
     "iopub.status.busy": "2021-07-06T13:35:53.990496Z",
     "iopub.status.idle": "2021-07-06T13:36:01.680228Z",
     "shell.execute_reply": "2021-07-06T13:36:01.679094Z",
     "shell.execute_reply.started": "2021-07-06T13:35:53.990803Z"
    },
    "id": "UQaajoPt2Awo"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "#https://pytorch.org/text/stable/vocab.html#vectors\n",
    "\n",
    "vectors_en = Vectors(name='../input/400ksentence35epoch/word2vec_en.wordvectors', cache='en_vec') # model_name + path = path_to_embeddings_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:07.844254Z",
     "iopub.status.busy": "2021-07-06T13:36:07.84393Z",
     "iopub.status.idle": "2021-07-06T13:36:22.865314Z",
     "shell.execute_reply": "2021-07-06T13:36:22.864435Z",
     "shell.execute_reply.started": "2021-07-06T13:36:07.844222Z"
    },
    "id": "FW7sFiTZ3HON"
   },
   "outputs": [],
   "source": [
    "vectors_ru = Vectors(name='../input/400ksentence35epoch/word2vec_ru.wordvectors', cache='ru_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:22.86821Z",
     "iopub.status.busy": "2021-07-06T13:36:22.867861Z",
     "iopub.status.idle": "2021-07-06T13:36:54.865033Z",
     "shell.execute_reply": "2021-07-06T13:36:54.864138Z",
     "shell.execute_reply.started": "2021-07-06T13:36:22.868172Z"
    },
    "id": "8FtaAOjQjU0y"
   },
   "outputs": [],
   "source": [
    "#https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field \n",
    "SRC = Field(tokenize=tokenize_ru,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first = True)\n",
    "\n",
    "# batch size is the first dimension\n",
    "TRG = Field(tokenize=tokenize_en,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first = True)\n",
    "\n",
    "# https://torchtext.readthedocs.io/en/latest/data.html#tabulardataset\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path='../input/400ksentence35epoch/data_ya_hw_3_modif.txt',\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:54.885648Z",
     "iopub.status.busy": "2021-07-06T13:36:54.88522Z",
     "iopub.status.idle": "2021-07-06T13:36:55.514012Z",
     "shell.execute_reply": "2021-07-06T13:36:55.513135Z",
     "shell.execute_reply.started": "2021-07-06T13:36:54.885611Z"
    },
    "id": "ezRiB-ReNDH7"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48504854/python-random-getstate-and-random-setstate/48505649\n",
    "random.seed(42)\n",
    "st = random.getstate()\n",
    "train_data, test_data, valid_data  = dataset.split(split_ratio=[0.8, 0.15, 0.05], random_state = st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:55.51575Z",
     "iopub.status.busy": "2021-07-06T13:36:55.51539Z",
     "iopub.status.idle": "2021-07-06T13:36:55.521703Z",
     "shell.execute_reply": "2021-07-06T13:36:55.520578Z",
     "shell.execute_reply.started": "2021-07-06T13:36:55.515712Z"
    },
    "id": "JinS0BvjOu_5",
    "outputId": "3ccfcaf7-104e-4f93-ff08-34201ed10a69"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:55.525419Z",
     "iopub.status.busy": "2021-07-06T13:36:55.524839Z",
     "iopub.status.idle": "2021-07-06T13:36:55.531271Z",
     "shell.execute_reply": "2021-07-06T13:36:55.530152Z",
     "shell.execute_reply.started": "2021-07-06T13:36:55.525376Z"
    },
    "id": "qMplC_l8b-Tt",
    "outputId": "935ee401-6e88-4a7f-ad5b-54cb95fab690"
   },
   "outputs": [],
   "source": [
    "print(f\"Obtained number of examples: {len(train_data.examples) + len(valid_data.examples) + len(test_data.examples)}\")\n",
    "print(f\"Expected number of examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:36:55.533563Z",
     "iopub.status.busy": "2021-07-06T13:36:55.533128Z",
     "iopub.status.idle": "2021-07-06T13:37:03.48984Z",
     "shell.execute_reply": "2021-07-06T13:37:03.488926Z",
     "shell.execute_reply.started": "2021-07-06T13:36:55.533503Z"
    },
    "id": "zpNiR4W7O0B5"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/62291303/pytorch-loading-word-vectors-into-field-vocabulary-vs-embedding-layer\n",
    "\n",
    "TRG.build_vocab(train_data, min_freq = 5, vectors=vectors_en)\n",
    "SRC.build_vocab(train_data, min_freq = 5, vectors=vectors_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:03.491492Z",
     "iopub.status.busy": "2021-07-06T13:37:03.491083Z",
     "iopub.status.idle": "2021-07-06T13:37:03.496702Z",
     "shell.execute_reply": "2021-07-06T13:37:03.495821Z",
     "shell.execute_reply.started": "2021-07-06T13:37:03.491449Z"
    },
    "id": "QV3a6tXZMiIK",
    "outputId": "8fa870b5-e122-470a-fbbb-a5015c010ab8"
   },
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:25.558471Z",
     "iopub.status.busy": "2021-07-06T13:37:25.55812Z",
     "iopub.status.idle": "2021-07-06T13:37:25.563167Z",
     "shell.execute_reply": "2021-07-06T13:37:25.561867Z",
     "shell.execute_reply.started": "2021-07-06T13:37:25.558433Z"
    },
    "id": "v4-wY4xVNXSE"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-03T21:23:12.429721Z",
     "iopub.status.busy": "2021-07-03T21:23:12.429334Z",
     "iopub.status.idle": "2021-07-03T21:23:12.433625Z",
     "shell.execute_reply": "2021-07-03T21:23:12.432583Z",
     "shell.execute_reply.started": "2021-07-03T21:23:12.429683Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "PSJSKbZCR75_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "NJjUtK9HVSo5"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:33.052527Z",
     "iopub.status.busy": "2021-07-06T13:37:33.05218Z",
     "iopub.status.idle": "2021-07-06T13:37:33.061853Z",
     "shell.execute_reply": "2021-07-06T13:37:33.06074Z",
     "shell.execute_reply.started": "2021-07-06T13:37:33.052494Z"
    },
    "id": "xL5lSMvxR5T_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # tokens embeddings\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim).from_pretrained(SRC.vocab.vectors)\n",
    "\n",
    "        # positional embeddings. max_length - maximal number of words in sentence\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # words indexing in positional embeddings (from 0 to src_len-1)\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        # we have to sum up pos and ordinary embeddings. Ordinary embeddings are normalized beforehand\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        # src_mask maps <pad> token to 0 and not <pad> token to 1\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:35.911041Z",
     "iopub.status.busy": "2021-07-06T13:37:35.910691Z",
     "iopub.status.idle": "2021-07-06T13:37:35.918239Z",
     "shell.execute_reply": "2021-07-06T13:37:35.91742Z",
     "shell.execute_reply.started": "2021-07-06T13:37:35.911007Z"
    },
    "id": "USHdv6Jtj5fV"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        # k, q и v получаются из src \n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "qJdRYFPIkYKq"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:38.275606Z",
     "iopub.status.busy": "2021-07-06T13:37:38.275231Z",
     "iopub.status.idle": "2021-07-06T13:37:38.289748Z",
     "shell.execute_reply": "2021-07-06T13:37:38.288775Z",
     "shell.execute_reply.started": "2021-07-06T13:37:38.275572Z"
    },
    "id": "u1AvPer1j8Iu"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "      \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            # <pad> tokens \"weight\" sets to near zero value\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        # .contiguous() - make result contigous\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "l32kKTY0kb1g"
   },
   "source": [
    "## FF layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:40.881939Z",
     "iopub.status.busy": "2021-07-06T13:37:40.881615Z",
     "iopub.status.idle": "2021-07-06T13:37:40.889231Z",
     "shell.execute_reply": "2021-07-06T13:37:40.887226Z",
     "shell.execute_reply.started": "2021-07-06T13:37:40.881908Z"
    },
    "id": "dTfAvkStj-k7"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "n-sxCiQKkCWP"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:43.060765Z",
     "iopub.status.busy": "2021-07-06T13:37:43.060424Z",
     "iopub.status.idle": "2021-07-06T13:37:43.070873Z",
     "shell.execute_reply": "2021-07-06T13:37:43.06981Z",
     "shell.execute_reply.started": "2021-07-06T13:37:43.060734Z"
    },
    "id": "pw5VeDGRkBIA"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim).from_pretrained(TRG.vocab.vectors)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "                            \n",
    "        #pos = [batch size, trg len]\n",
    "\n",
    "        # see https://discuss.pytorch.org/t/google-colab-runtimeerror-cuda-error-device-side-assert-triggered/69559    \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "                \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:45.556776Z",
     "iopub.status.busy": "2021-07-06T13:37:45.556452Z",
     "iopub.status.idle": "2021-07-06T13:37:45.565811Z",
     "shell.execute_reply": "2021-07-06T13:37:45.564797Z",
     "shell.execute_reply.started": "2021-07-06T13:37:45.556746Z"
    },
    "id": "pNFp-7DrkF4Q"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #decoder self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        \n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        # attention between encoder and decoder\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "GiB8tej_kKHk"
   },
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:47.71927Z",
     "iopub.status.busy": "2021-07-06T13:37:47.718956Z",
     "iopub.status.idle": "2021-07-06T13:37:47.728076Z",
     "shell.execute_reply": "2021-07-06T13:37:47.727275Z",
     "shell.execute_reply.started": "2021-07-06T13:37:47.71924Z"
    },
    "id": "GDCEas_YkIkl"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "TeUj9nPvlY7B"
   },
   "source": [
    "# Training routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "SdyvcCrGlbQU"
   },
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:52.428118Z",
     "iopub.status.busy": "2021-07-06T13:37:52.427792Z",
     "iopub.status.idle": "2021-07-06T13:37:57.136185Z",
     "shell.execute_reply": "2021-07-06T13:37:57.135218Z",
     "shell.execute_reply.started": "2021-07-06T13:37:52.428086Z"
    },
    "id": "E7tdAZykkPCg"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 4\n",
    "DEC_LAYERS = 4\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:57.13798Z",
     "iopub.status.busy": "2021-07-06T13:37:57.137633Z",
     "iopub.status.idle": "2021-07-06T13:37:57.201101Z",
     "shell.execute_reply": "2021-07-06T13:37:57.200338Z",
     "shell.execute_reply.started": "2021-07-06T13:37:57.137942Z"
    },
    "id": "hvgqc82yljzk"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:37:59.023378Z",
     "iopub.status.busy": "2021-07-06T13:37:59.023029Z",
     "iopub.status.idle": "2021-07-06T13:37:59.030651Z",
     "shell.execute_reply": "2021-07-06T13:37:59.029677Z",
     "shell.execute_reply.started": "2021-07-06T13:37:59.023325Z"
    },
    "id": "7eOO80Pblu1q",
    "outputId": "def7f95c-07f2-466b-82d5-d0c41e2c91e7"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "sXrxYYf0mLpQ"
   },
   "source": [
    "Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:38:05.96711Z",
     "iopub.status.busy": "2021-07-06T13:38:05.966783Z",
     "iopub.status.idle": "2021-07-06T13:38:05.971215Z",
     "shell.execute_reply": "2021-07-06T13:38:05.970417Z",
     "shell.execute_reply.started": "2021-07-06T13:38:05.96708Z"
    },
    "id": "j4GoIqNllxNb"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1 and type(m) != nn.Embedding:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-03T21:23:17.793706Z",
     "iopub.status.busy": "2021-07-03T21:23:17.793327Z",
     "iopub.status.idle": "2021-07-03T21:23:17.806593Z",
     "shell.execute_reply": "2021-07-03T21:23:17.805662Z",
     "shell.execute_reply.started": "2021-07-03T21:23:17.793671Z"
    },
    "id": "0WInLjwbl0fN"
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "tBZOXgLEmQVO"
   },
   "source": [
    "Set learning rate and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:38:32.465257Z",
     "iopub.status.busy": "2021-07-06T13:38:32.464908Z",
     "iopub.status.idle": "2021-07-06T13:38:32.475277Z",
     "shell.execute_reply": "2021-07-06T13:38:32.474446Z",
     "shell.execute_reply.started": "2021-07-06T13:38:32.465214Z"
    },
    "id": "AqZtuvLKmJz7"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "LEARNING_RATE = 1e-4 # initial lr=5e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "#https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.3, patience = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:38:34.82031Z",
     "iopub.status.busy": "2021-07-06T13:38:34.819984Z",
     "iopub.status.idle": "2021-07-06T13:38:34.824207Z",
     "shell.execute_reply": "2021-07-06T13:38:34.823297Z",
     "shell.execute_reply.started": "2021-07-06T13:38:34.820278Z"
    },
    "id": "LHODgklmmKS6"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "2M4n5JHomyBS"
   },
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:38:38.425697Z",
     "iopub.status.busy": "2021-07-06T13:38:38.425328Z",
     "iopub.status.idle": "2021-07-06T13:38:38.436248Z",
     "shell.execute_reply": "2021-07-06T13:38:38.435449Z",
     "shell.execute_reply.started": "2021-07-06T13:38:38.425665Z"
    },
    "id": "b4UfuvAQmWOm"
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    history = []\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # we do not need <eos> here\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # we ignore <eos> during loss calculation    \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "\n",
    "        #set_trace()    \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        history.append(loss.cpu().data.numpy())\n",
    "        if (i+1)%100==0:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "            clear_output(True)\n",
    "            ax[0].plot(history, label='train loss')\n",
    "            ax[0].set_xlabel('Batch')\n",
    "            ax[0].set_title('Train loss')\n",
    "            if train_history is not None:\n",
    "                ax[1].plot(train_history, label='general train history')\n",
    "                ax[1].set_xlabel('Epoch')\n",
    "            if valid_history is not None:\n",
    "                ax[1].plot(valid_history, label='general valid history')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "Xibd-CX-m22v"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:38:41.742568Z",
     "iopub.status.busy": "2021-07-06T13:38:41.742172Z",
     "iopub.status.idle": "2021-07-06T13:38:41.749412Z",
     "shell.execute_reply": "2021-07-06T13:38:41.748206Z",
     "shell.execute_reply.started": "2021-07-06T13:38:41.742532Z"
    },
    "id": "1udrbM-Vm0UG"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T14:03:47.31644Z",
     "iopub.status.busy": "2021-07-06T14:03:47.3161Z",
     "iopub.status.idle": "2021-07-06T14:03:47.32204Z",
     "shell.execute_reply": "2021-07-06T14:03:47.320825Z",
     "shell.execute_reply.started": "2021-07-06T14:03:47.316408Z"
    },
    "id": "JF5KOEBYm6Hz"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "qSGJNHiFnKOV"
   },
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:39:32.064954Z",
     "iopub.status.busy": "2021-07-06T13:39:32.064616Z",
     "iopub.status.idle": "2021-07-06T13:39:44.909388Z",
     "shell.execute_reply": "2021-07-06T13:39:44.908372Z",
     "shell.execute_reply.started": "2021-07-06T13:39:32.064924Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../input/400ksentence35epoch/best-val-model_35_epoch.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T13:39:50.316615Z",
     "iopub.status.busy": "2021-07-06T13:39:50.316246Z",
     "iopub.status.idle": "2021-07-06T13:47:13.596386Z",
     "shell.execute_reply": "2021-07-06T13:47:13.595461Z",
     "shell.execute_reply.started": "2021-07-06T13:39:50.316583Z"
    }
   },
   "outputs": [],
   "source": [
    "_, _, bleu_score = calculate_bleu_alt(test_iterator, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T14:03:54.17812Z",
     "iopub.status.busy": "2021-07-06T14:03:54.177775Z",
     "iopub.status.idle": "2021-07-06T17:24:01.936237Z",
     "shell.execute_reply": "2021-07-06T17:24:01.933468Z",
     "shell.execute_reply.started": "2021-07-06T14:03:54.178087Z"
    },
    "id": "WJICvCNwm80U",
    "outputId": "ecd7d90b-7d83-4d9a-ec71-bdf56a0a71a6"
   },
   "outputs": [],
   "source": [
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-model.pt')\n",
    "    \n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:24:10.793595Z",
     "iopub.status.busy": "2021-07-06T17:24:10.793236Z",
     "iopub.status.idle": "2021-07-06T17:24:19.062012Z",
     "shell.execute_reply": "2021-07-06T17:24:19.061086Z",
     "shell.execute_reply.started": "2021-07-06T17:24:10.793562Z"
    },
    "id": "YP-sJCk9nPg-"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-val-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "6Votm0m0nZsN"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "oib41tuvoJRN"
   },
   "source": [
    "## One sentence translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:24:34.269593Z",
     "iopub.status.busy": "2021-07-06T17:24:34.269233Z",
     "iopub.status.idle": "2021-07-06T17:24:34.279176Z",
     "shell.execute_reply": "2021-07-06T17:24:34.278384Z",
     "shell.execute_reply.started": "2021-07-06T17:24:34.269561Z"
    },
    "id": "egDXQpFFnWuS"
   },
   "outputs": [],
   "source": [
    "# one sentence translate\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "aWB4rRhKoMTh"
   },
   "source": [
    "## BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:24:36.360925Z",
     "iopub.status.busy": "2021-07-06T17:24:36.36061Z",
     "iopub.status.idle": "2021-07-06T17:24:36.370388Z",
     "shell.execute_reply": "2021-07-06T17:24:36.369537Z",
     "shell.execute_reply.started": "2021-07-06T17:24:36.360895Z"
    },
    "id": "mfIpjsirnqhr"
   },
   "outputs": [],
   "source": [
    "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "\n",
    "    model.eval()\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "\n",
    "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    translations_done = [0] * len(src_tensor)\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        pred_tokens = output.argmax(2)[:,-1]\n",
    "        for i, pred_token_i in enumerate(pred_tokens):\n",
    "            trg_indexes[i].append(pred_token_i)\n",
    "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "                translations_done[i] = 1\n",
    "        if all(translations_done):\n",
    "            break\n",
    "\n",
    "    # Iterate through each predicted example one by one;\n",
    "    # Cut-off the portion including the after the <eos> token\n",
    "    pred_sentences = []\n",
    "    for trg_sentence in trg_indexes:\n",
    "        pred_sentence = []\n",
    "        for i in range(1, len(trg_sentence)):\n",
    "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "                break\n",
    "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
    "        pred_sentences.append(pred_sentence)\n",
    "\n",
    "    return pred_sentences, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:24:38.027898Z",
     "iopub.status.busy": "2021-07-06T17:24:38.027578Z",
     "iopub.status.idle": "2021-07-06T17:24:38.03562Z",
     "shell.execute_reply": "2021-07-06T17:24:38.034637Z",
     "shell.execute_reply.started": "2021-07-06T17:24:38.027867Z"
    },
    "id": "1ZdLrI0zrF6u"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            _trgs = []\n",
    "            for sentence in trg:\n",
    "                tmp = []\n",
    "                # Start from the first token which skips the <start> token\n",
    "                for i in sentence[1:]:\n",
    "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
    "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
    "                        break\n",
    "                    tmp.append(trg_field.vocab.itos[i])\n",
    "                _trgs.append([tmp])\n",
    "            trgs += _trgs\n",
    "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
    "            for sent in pred_trg:\n",
    "                sent = [token for token in sent if token != '<unk>']\n",
    "            pred_trgs += pred_trg\n",
    "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "qSme29ysrGcr"
   },
   "outputs": [],
   "source": [
    "_, _, bleu_score = calculate_bleu_alt(test_iterator, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:32:06.749892Z",
     "iopub.status.busy": "2021-07-06T17:32:06.749565Z",
     "iopub.status.idle": "2021-07-06T17:32:06.755003Z",
     "shell.execute_reply": "2021-07-06T17:32:06.754162Z",
     "shell.execute_reply.started": "2021-07-06T17:32:06.749861Z"
    },
    "id": "zpfZF6dErIXU"
   },
   "outputs": [],
   "source": [
    "example_idx = 158\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T17:32:28.094136Z",
     "iopub.status.busy": "2021-07-06T17:32:28.093819Z",
     "iopub.status.idle": "2021-07-06T17:32:28.210979Z",
     "shell.execute_reply": "2021-07-06T17:32:28.210064Z",
     "shell.execute_reply.started": "2021-07-06T17:32:28.094107Z"
    },
    "id": "JqFyYizd62_G"
   },
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2021-07-06T18:03:05.693323Z",
     "iopub.status.busy": "2021-07-06T18:03:05.693003Z",
     "iopub.status.idle": "2021-07-06T18:03:05.7783Z",
     "shell.execute_reply": "2021-07-06T18:03:05.777292Z",
     "shell.execute_reply.started": "2021-07-06T18:03:05.693293Z"
    },
    "id": "fKDA6kVdMIGE"
   },
   "outputs": [],
   "source": [
    "src=tokenize_ru('Сегодня я не планирую обучать глубокие сети.')\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
